{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b31ed477",
   "metadata": {},
   "source": [
    "# Feature Engineering and Classification of Disaster Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f20923b",
   "metadata": {},
   "source": [
    "##### Notes to self\n",
    "* make sure to explore the feature data via plotting before and after reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc4ec2",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646ae2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# temporarily set this environment variable to avoid issues with duplicate libraries when using Pytorch\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "home = os.path.expanduser(\"~\")\n",
    "this_dir = os.path.join(home, \"SURE-2025\")\n",
    "os.chdir(this_dir) # ensure we are in the correct directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edad7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting seeds and the device\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 9\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\", torch.cuda.current_device())\n",
    "    print('using cuda')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('using cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1970c",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32d34b",
   "metadata": {},
   "source": [
    "In this project, we are using the CrisisMMD dataset. There are several advantages of using CrisisMMD. \n",
    "#### Advantages\n",
    "* each tweet has at least one image associated with it, so we can train text and image models that work on the same distribution\n",
    "* ***continue later***\n",
    "\n",
    "But there are some disadvantages worth mentioning as well. \n",
    "#### Disadvantages\n",
    "* the humanitarian classes we use here are highly imbalanced\n",
    "* images and tweets can have different labels, which can complicate classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7021e",
   "metadata": {},
   "source": [
    "We can begin visualizing the data by placing it into a pandas DataFrame. After, we will place it in a custom Pytorch Dataset. \n",
    "\n",
    "**Note**: after downloading CrisisMMD v2, I placed it directly in my user folder for the most direct access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# access the data from the folders\n",
    "home = Path(os.path.expanduser('~'))\n",
    "dir = home / 'CrisisMMD_v2.0' / 'CrisisMMD_v2.0' \n",
    "datasplit_dir = dir / 'crisismmd_datasplit_all' / 'crisismmd_datasplit_all'\n",
    "train_data = pd.read_csv(datasplit_dir / 'task_humanitarian_text_img_train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d152aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1029c2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['label_image'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e24b2a",
   "metadata": {},
   "source": [
    "Now that we know what the data looks like, we can visualize the images by accessing the paths in the DataFrame. (*Warning*: the dataset contains some sensitive images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911bb251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize 6 images from the training set of the classes we are interested in\n",
    "classes = [label for label in \n",
    "               train_data['label_image'].value_counts().reset_index()['label_image'].to_dict().values()\n",
    "               if label not in ['vehicle_damage', 'missing_or_found_people']]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(10, 5))\n",
    "# i goes 0 through 5\n",
    "for i, label in enumerate(classes):\n",
    "    ax = axes[i // 3, i % 3]\n",
    "    img_path = train_data[train_data['label_image'] == label].iloc[0]['image']\n",
    "    img = Image.open(dir / img_path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(label)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669329e5",
   "metadata": {},
   "source": [
    "## Feature Engineering - Traditional Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37991a03",
   "metadata": {},
   "source": [
    "We will want to see how several different combinations of transformations affect the images. Those transformations are Gabor filters, Hu Moments, color histograms, and local binary patterns. These are considered traditional image feature-engineering techniques. The goal with applying the transformations is to promote intra-class similarity and inter-class variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e9312",
   "metadata": {},
   "source": [
    "### Gabor filters\n",
    "\n",
    "Gabor filters are meant to mimic the way that humans see images. They are commonly used for feature discrimination, texture analysis, and edge detection. They represent a Gaussian envelope modified by a sinusoidal function. They have been shown to have good results on complex tasks, such as facial images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b32844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage import data\n",
    "from skimage.util import img_as_float\n",
    "from skimage.filters import gabor_kernel\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# TODO: learn more about optimal params and features to extract from gabor filters\n",
    "# computes features that can be input to a classifier\n",
    "def compute_gabor_feats(image, kernels):\n",
    "    feats = np.zeros((len(kernels), 4), dtype=np.double)\n",
    "    for i, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(image, kernel, mode='wrap')\n",
    "        feats[i, 0] = filtered.mean()\n",
    "        feats[i, 1] = filtered.var()\n",
    "        feats[i, 3] = np.linalg.norm(filtered) # computes the L2 norm/magnitude of the filtered image\n",
    "        feats[i, 2] = filtered.std()\n",
    "    return feats\n",
    "\n",
    "# for convolving the gabor kernel with the images to see response\n",
    "# input is a ndarray image and a kernel\n",
    "def power(image, kernel):\n",
    "    # Normalize (and convert to greyscale) images for better comparison.\n",
    "    # The range of a floating point image is [0.0, 1.0] or [-1.0, 1.0] \n",
    "    image = img_as_float(rgb2gray(image))\n",
    "    image = (image - image.mean()) / image.std()\n",
    "    return np.sqrt(ndi.convolve(input=image, weights=np.real(kernel), mode='wrap') **2\n",
    "        + ndi.convolve(image, np.imag(kernel), mode='wrap') ** 2)*255\n",
    "\n",
    "\n",
    "# example kernel creation \n",
    "\n",
    "kernels = []\n",
    "for theta in range(4):\n",
    "    theta = theta / 4.0 * np.pi \n",
    "    for sigma in (1,3):\n",
    "        for freq in (0.05,.25):\n",
    "            kernel = np.real(\n",
    "                gabor_kernel(freq, theta=theta, sigma_x=sigma, sigma_y=sigma)\n",
    "            )\n",
    "            kernels.append(kernel)\n",
    "\n",
    "for kernel in kernels:\n",
    "    print(kernel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "shrink = (slice(0, None, 3), slice(0, None, 3)) # to shrink image size before using the kernel\n",
    "imgs_ = []\n",
    "for i, label in enumerate(classes):\n",
    "    img_path = train_data[train_data['label_image'] == label].iloc[0]['image']\n",
    "    img = Image.open(dir / img_path)\n",
    "    \n",
    "    img = img_as_float(np.array(img)[shrink])\n",
    "    imgs_.append(img)\n",
    "\n",
    "\n",
    "powers = []\n",
    "for img in imgs_:\n",
    "    for kernel in kernels:\n",
    "        powers.append(power(img, kernel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b34e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of img transformed by kernel\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,6))\n",
    "ax.set_axis_off()\n",
    "ax.imshow(powers[48], cmap='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "label = classes[3] # rescue, volunteer, or donation\n",
    "\n",
    "image = train_data[train_data['label_image'] == label].iloc[0]['image']\n",
    "\n",
    "image = cv2.normalize(rgb2gray(np.array(Image.open(dir / image))), None, 0, 1, cv2.NORM_MINMAX)\n",
    "\n",
    "feats = compute_gabor_feats(image, kernels)\n",
    "\n",
    "# feats describing mean, variance, magnitude, and standard deviation of the filtered image\n",
    "# each row is a kernel, each column is a feature\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e728ca1",
   "metadata": {},
   "source": [
    "### Hu Moments\n",
    "Image moments are the weighted average of pixel intensities. For a binary image (black and white), it would just be the number of white pixels. Central moments are similar but they are translation invariant, so they remain the same regardless of where a shape occurs in the image. From this, Hu Moments are calculated.  \n",
    "\n",
    "[Hu Moments](https://learnopencv.com/shape-matching-using-hu-moments-c-python/) are a set of 7 numbers that are invariant (don't change) to image transformations. The first 6 moments have been proved to be invariant to translation, scale, and rotation, and reflection. While the 7th momentâ€™s sign changes for image reflection.\n",
    "\n",
    "Because each of the moments are of very different scales, a log transform should be used to bring them to a comparable range. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3298882b",
   "metadata": {},
   "source": [
    "### Color Histograms\n",
    "\n",
    "Color histograms represent the distribution of colors in an image. \n",
    "\n",
    "A tutorial for visualizing color histograms can be found [here](https://pyimagesearch.com/2021/04/28/opencv-image-histograms-cv2-calchist/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf4c9f8",
   "metadata": {},
   "source": [
    "##### 1D Color Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a181c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = np.array(Image.open(dir / train_data[train_data['label_image'] == \n",
    "                                 'affected_individuals'].iloc[0]['image']\n",
    "                                 ).resize((224, 224))).astype(np.uint8)\n",
    "\n",
    "test_cv2 = Image.fromarray(test_img)\n",
    "test_cv2.show()\n",
    "\n",
    "test_img = cv2.normalize(test_img, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "chans = cv2.split(test_img) # b g r\n",
    "colors = ['b', 'g', 'r']\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Flattened Color Histogram')\n",
    "plt.xlabel('Bins')\n",
    "plt.ylabel('# of Pixels')\n",
    "\n",
    "\n",
    "\n",
    "for (chan, color) in zip(chans, colors):\n",
    "    hist = cv2.calcHist([chan], channels=[0], mask=None, histSize=[256], ranges=[0,256])\n",
    "    plt.plot(hist, color=color)\n",
    "    plt.xlim([0, 256])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e837e",
   "metadata": {},
   "source": [
    "##### 2D Color Histograms\n",
    "2D color histograms will show us how many pixels have X number of color1 and Y number of color2 pixels, for example. \n",
    "\n",
    "**Note**: We used 256 bins above to best visualize the pixel intensity per channel, but we shorten it to 32 bins for smaller visualizations below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors: b, g, r\n",
    "\n",
    "fig = plt.figure(figsize=(30, 30))\n",
    "ax = fig.add_subplot(2, 6, 1)\n",
    "\n",
    "# green and blue\n",
    "hist = cv2.calcHist([chans[1], chans[0]], [0, 1], None, [32, 32], [0,256]*2)\n",
    "p = ax.imshow(hist, interpolation='nearest', cmap='pink')\n",
    "ax.set_title('2D Color Hist for G and B')\n",
    "plt.colorbar(p, shrink=.4)\n",
    "\n",
    "\n",
    "# green and red\n",
    "ax = fig.add_subplot(2, 6, 3)\n",
    "hist = cv2.calcHist([chans[1], chans[2]], [0, 1], None, [32, 32], [0, 256]*2)\n",
    "p = ax.imshow(hist, interpolation='nearest', cmap='pink')\n",
    "ax.set_title('2D Color Hist for G and R')\n",
    "plt.colorbar(p, shrink=.4)\n",
    "\n",
    "\n",
    "# red and blue\n",
    "ax = fig.add_subplot(2, 6, 5)\n",
    "hist = cv2.calcHist([chans[2], chans[0]], [0, 1], None, [32, 32], [0, 256]*2)\n",
    "p = ax.imshow(hist, interpolation='nearest', cmap='pink')\n",
    "ax.set_title('2D Color Hist for G and R')\n",
    "plt.colorbar(p, shrink=.4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c8766",
   "metadata": {},
   "source": [
    "### Local Binary Patterns\n",
    "\n",
    "LBPs are a powerful texture descriptor. They work by comparing the intensity of a central pixel with the intensity of surrounding pixels. It is efficient and invariant to rotation and scale. However it is sensitive to noise in images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f941ef",
   "metadata": {},
   "source": [
    "As explained on the scikit-learn documentation: \n",
    "<p style=\"text-align:center; font-size: 18px\">\"When using LBP to detect texture, you measure a collection of LBPs over an image patch and look at the distribution of these LBPs.\"</p>\n",
    "\n",
    "The histogram produced from LBP can be fed into a model as a feature representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832aeaf1",
   "metadata": {},
   "source": [
    "### Choosing the Best Combination of Features\n",
    "\n",
    "To find out which of the above techniques or combination of techniques best promotes intra-class similarity and inter-class variance, we need to find all the possible combinations and test each of them out on the images using a distance metric on the resulting image matrices. The distance metric we will use is Euclidean distance. \n",
    "$$Euclidean\\ Distance = |X - Y| = \\sqrt{\\sum_{i=1}^{i=n}(x_i - y_i)^2}$$\n",
    "\n",
    "The code below shows the preliminary functions necessary to finding the best features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Function used to find all possible (unordered) combinations\n",
    "def powerset_without_emptyset(items):\n",
    "    '''\n",
    "    Returns the powerset of a list of items as a list of tuples, excluding the empty set\n",
    "    The powerset is the set of all subsets of a set \n",
    "    \n",
    "    Example output\n",
    "    powerset_without_emptyset([1, 2, 3])\n",
    "    returns: [(1,), (2,), (3,), (1, 2), (1, 3), (2, 3), (1, 2, 3)]\n",
    "    '''\n",
    "    combos = []\n",
    "    for i in range(len(items)):\n",
    "        combos.extend(list(combinations(items, len(items) - i)))\n",
    "    return combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f266092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import skimage.feature\n",
    "\n",
    "# example transforms to use\n",
    "transforms = [cv2.HuMoments, skimage.feature.graycomatrix, cv2.calcHist, skimage.feature.local_binary_pattern]\n",
    "all_combos = powerset_without_emptyset(transforms)\n",
    "\n",
    "# total number of combinations\n",
    "len(all_combos)\n",
    "all_combos[:5]  # Display the first 5 combinations\n",
    "\n",
    "\n",
    "for combo in all_combos:\n",
    "    if len(combo) == 1:\n",
    "        print(f\"Single transform: {combo[0].__name__}\")\n",
    "\n",
    "print(f'\\nTotal number of combinations: {len(all_combos)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02459160",
   "metadata": {},
   "source": [
    "Now we have to test all 15 combinations on the CrisisMMD images. To do this, we will use contrastive loss that uses Euclidean Distance to compare results. The combination that results will be the lowest contrastive loss is the combination that best distinguishes classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6b752",
   "metadata": {},
   "source": [
    "The formula for contrastive loss is: \n",
    "$$L = \\sum_{i=1}^{P} L\\left(W, (Y, X_1, X_2)^i\\right)$$\n",
    "\n",
    "$$L\\left(W, (Y, X_1, X_2)^i\\right) = (1 - Y) \\, L_G\\left(E_W(X_1, X_2)^i\\right) + Y \\, L_I\\left(E_W (X_1, X_2)^i\\right)$$ \n",
    "\n",
    "where $L_G$ is a loss function used for when the pairs are from the same class and $L_I$ is for when the pairs are not from the same class. \n",
    "\n",
    "This can then become:\n",
    "$$\n",
    "L = [y_i = y_j] \\|f(x_i) - f(x_j)\\|_2^2 + [y_i \\neq y_j] \\max\\left(0, \\varepsilon - \\|f(x_i) - f(x_j)\\|_2\\right)^2\n",
    "$$\n",
    "\n",
    "That formula penalizes pairs of examples for being either too far apart (if in the same class) or too similar (if in different classes).\n",
    "\n",
    "**Note**: the L2 norm (seen in the contrastive loss formula) is equivalent to Euclidean Distance. \n",
    "\n",
    "Sources: \n",
    "* https://lilianweng.github.io/posts/2021-05-31-contrastive/\n",
    "*  Paper \"Learning a Similarity Metric Discriminatively, with Application to Face\n",
    "Verification\" (Chopra et al. 2005): https://www.cs.utoronto.ca/~hinton/csc2535_06/readings/chopra-05.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3cebf7",
   "metadata": {},
   "source": [
    "#### Custom Contrastive Loss Function\n",
    "\n",
    "Below is an implementation of contrastive loss that we can use on pairs of images in order to see which transformations result in the most overall inter-class variance and intra-class similarity. It can also, potentially, be used to find which settings for the transformations are ideal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa7b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics.pairwise import nan_euclidean_distances\n",
    "\n",
    "#TODO: find a way to further vectorize this function so you won't have to loop through each pair of images\n",
    "class ContrastiveLossPairs(nn.Module): \n",
    "    def __init__(self, margin=.5):\n",
    "        super(ContrastiveLossPairs, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def _forward(self, pair, labels):\n",
    "        \"\"\"\n",
    "        pair - a pair of images (tensor of shape (2, C, H, W))\n",
    "        labels - a tensor of shape (2,) containing the labels of the images in the pair\n",
    "        \"\"\"\n",
    "       \n",
    "        loss = torch.tensor(0.0)\n",
    "        if labels[0] == labels[1]:  \n",
    "            loss += nan_euclidean_distances(pair[0] - pair[1])**2\n",
    "        else: \n",
    "            loss += torch.max(torch.tensor(0.0), self.margin - nan_euclidean_distances(pair[0] - pair[1]))**2\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, pairs, labels):\n",
    "        '''\n",
    "        pairs - a tensor of shape (N, 2, C, H, W) where N is the number of pairs\n",
    "        labels - a tensor of shape (N, 2) containing the labels of the images in the pairs\n",
    "        '''\n",
    "        X1 = pairs[:,0, :, :, :] # the first images per pair\n",
    "        X2 = pairs[:,1, :, :, :]\n",
    "        labels1 = labels[:, 0]\n",
    "        labels2 = labels[:, 1]\n",
    "\n",
    "        loss = (labels1==labels2).float() * nan_euclidean_distances(X1, X2)**2 + \\\n",
    "        (labels1!=labels2).float() * torch.max(torch.tensor(0.0), self.margin - nan_euclidean_distances(X1, X2))**2\n",
    "\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d1ead",
   "metadata": {},
   "source": [
    "#### Applying Image Transformations\n",
    "\n",
    "The next step is to apply the transformations to the images so that we can have a matrix of feature representation per group of tranformations, where each feature representation represents an image.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b9f20",
   "metadata": {},
   "source": [
    "**NOTE TO SELF**: these take in normalized images but do not normalize or flatten their output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18086651",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: make these transformation functions more customizable\n",
    "# TODO: change the transforms to these functions\n",
    "# TODO: check if cv2.normalize converts imgs to greyscale - we do NOT want that for color hists\n",
    "\n",
    "def apply_HuMoments(image):\n",
    "    '''\n",
    "    Computes and returns the Hu Moments of an image.\n",
    "    Parameters:\n",
    "    - image: a normalized numpy array representing the image\n",
    "    Returns:\n",
    "    - feat: an unnormalized numpy array of Hu Moments\n",
    "    '''\n",
    "    # Convert to grayscale if the image is RGB\n",
    "    assert(img.ndim <= 3), \"Image must be a 2D or 3D array\"\n",
    "\n",
    "    if image.ndim == 3:\n",
    "        im_grey = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        im_grey = image\n",
    "    # compute moments\n",
    "    feat = cv2.HuMoments(cv2.moments(im_grey)).flatten()\n",
    "\n",
    "    return feat\n",
    "\n",
    "# create kernels\n",
    "def make_gabor_kernels(n_theta=4, sigma_range=(1,3), freq_range=(.05, .25)):\n",
    "    kernels = []\n",
    "    for theta in range(n_theta):\n",
    "        # TODO: double check that theta is in range correct range\n",
    "        theta = theta / n_theta * np.pi \n",
    "        for sigma in sigma_range:\n",
    "            for freq in freq_range:\n",
    "                kernel = np.real(\n",
    "                    gabor_kernel(freq, theta=theta, sigma_x=sigma, sigma_y=sigma)\n",
    "                )\n",
    "                kernels.append(kernel)\n",
    "    return kernels\n",
    "\n",
    "def apply_Gabor(image):\n",
    "    assert(img.ndim <= 3), \"Image must be a 2D or 3D array\"\n",
    "    # convert to grayscale if the image is RGB\n",
    "    if image.ndim == 3:\n",
    "        im_grey = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        im_grey = image\n",
    "    kernels = make_gabor_kernels()\n",
    "\n",
    "    # compute features for each kernel\n",
    "    feats = compute_gabor_feats(im_grey, kernels).flatten()\n",
    "\n",
    "    return feats\n",
    "\n",
    "def apply_colorHist(image):\n",
    "    assert(img.ndim <= 3), \"Image must be a 2D or 3D array\"\n",
    "    if image.ndim == 2:\n",
    "        im_rgb = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        im_rgb = image\n",
    "    # compute color histogram  (256 is used b/c end of range is exclusive)\n",
    "    hist = cv2.calcHist([img], channels=[0,1,2], mask=None, histSize=[8,8,8], ranges=[0,256]*3)\n",
    "\n",
    "    return hist\n",
    "\n",
    "def apply_LBP(image, radius, n_points, method='uniform'):\n",
    "    assert(img.ndim <= 3), \"Image must be a 2D or 3D array\"\n",
    "    im_grey = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_RGB2GRAY) if img.ndim == 3 else img\n",
    "    \n",
    "    lbp = skimage.feature.local_binary_pattern(im_grey, P=n_points, R=radius, method=method)\n",
    "\n",
    "    n_bins = int(lbp.max()+1)\n",
    "\n",
    "    hist, _ = np.histogram(lbp.ravel(), density=True, bins=n_bins, range=(0, n_bins))\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformations(images, combo, shrink_by = 0):\n",
    "    \"\"\"\n",
    "    Applies a combination of transformations to a list of images.\n",
    "    Returns a 2D array: (num_images, total_feature_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    all_feats = []\n",
    "\n",
    "    # apply each transformation in combo to each image\n",
    "    for img in images:\n",
    "        # shrink image if specified\n",
    "        if shrink_by > 0:\n",
    "            if img.ndim == 2:\n",
    "                shrink = (slice(0, None, shrink_by), slice(0, None, shrink_by))\n",
    "            elif img.ndim == 3:\n",
    "                shrink = (slice(0, None, shrink_by), slice(0, None, shrink_by), slice(None))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported image dimensions: {img.ndim}. Expected 2D or 3D array.\")\n",
    "            \n",
    "            img = img[shrink]\n",
    "            \n",
    "        # ensure all imgs are in the same range \n",
    "        img = cv2.normalize(img, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "        img_feats = []\n",
    "        for t in combo:\n",
    "            if t is apply_HuMoments: \n",
    "                feat = apply_HuMoments(img)\n",
    "            elif t is apply_Gabor:\n",
    "                feat = apply_Gabor(img)\n",
    "            elif t is apply_colorHist:\n",
    "                feat = apply_colorHist(img)\n",
    "            elif t is apply_LBP:\n",
    "                feat = apply_LBP(img)\n",
    "            else: \n",
    "                print(t)\n",
    "                raise ValueError(f'Unsupported transformation: {t}')\n",
    "            \n",
    "            # flatten to ensure that the feature is a 1D array\n",
    "            # normalized to [0,1] as typically expected by a model\n",
    "            feat = cv2.normalize(feat.flatten(), None, 0, 1, cv2.NORM_MINMAX)\n",
    "\n",
    "            img_feats.append(feat)\n",
    "\n",
    "        # 1 feature representation per image\n",
    "        all_feats.append(np.concatenate(img_feats))\n",
    "    \n",
    "    \n",
    "    all_feats = np.stack(all_feats)\n",
    "    assert images.shape[0] == all_feats.shape[0], \"implementation is wrong after stacking\"\n",
    "\n",
    "    return all_feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a658f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_reps(transformations, images, labels):\n",
    "    '''\n",
    "    Returns:\n",
    "    - a list of dictionaries, each containing:\n",
    "        - 'combo_name': a string representing the combination of transformations applied\n",
    "        - 'feats': a 2D numpy array of shape (N, F) where N is the number of images and F is the size of the feature representation of each image\n",
    "        - 'labels': a 1D numpy array of shape (N,) containing the labels\n",
    "    '''\n",
    "    dict_list = []\n",
    "    combos = powerset_without_emptyset(transformations)\n",
    "    for combo in combos: \n",
    "        combo_name = '_'.join([t.__name__ for t in combo])\n",
    "\n",
    "        feats = apply_transformations(images, combo)\n",
    "\n",
    "        temp_dict = {\n",
    "            'combo_name': combo_name, \n",
    "            'feats': feats,\n",
    "            'labels': labels\n",
    "            }\n",
    "        \n",
    "        dict_list.append(temp_dict)\n",
    "\n",
    "        assert len(dict_list) == len(combos), \"dict_list length should match number of combos\"\n",
    "\n",
    "    return dict_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c98c9c4",
   "metadata": {},
   "source": [
    "##### Finding the best transformations to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56512b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of images from the dataframe\n",
    "\n",
    "# get a list of the labels from the dataframe\n",
    "\n",
    "# for each transformation, get the feature representations of the images and the labels\n",
    "transforms = [apply_colorHist, apply_Gabor, apply_HuMoments, apply_LBP]\n",
    "dict_list = get_feature_reps(transforms, images=..., labels=...)\n",
    "\n",
    "# place the images into pairings\n",
    "\n",
    "# run contrastive loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ece73e",
   "metadata": {},
   "source": [
    "## Feature Engineering - Deep Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51ff04",
   "metadata": {},
   "source": [
    "To extract \"deep\" features from images, we will run our CrisisMMD images through a deep CNN. The CNN we use is VGG16, pretrained on images from the Places365 dataset. \n",
    "\n",
    "We use the Places365 dataset because it is used for scene recognition tasks, and the important disaster-related images are often complex scenes containing different objects and people. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff25ae2",
   "metadata": {},
   "source": [
    "**Credit**: the downloadable weights can be found [here](https://github.com/GKalliatakis/Keras-VGG16-places365#) and are converted to Pytorch-compatible format by first converting the Tensorflow model to ONNX format. See tutorial [here](https://www.geeksforgeeks.org/deep-learning/how-to-convert-a-tensorflow-model-to-pytorch/). \n",
    "\n",
    "\n",
    "**Note**: After converting the formats, the output of the models was tested using randomly generated using inputs created from torch.randn and tf.rand.normal. The cosine similarity between the flattened output was .9983, strongly supporting that the conversion was successful. \n",
    "\n",
    "**Note from [documentation](https://github.com/CSAILVision/places365/tree/master?tab=readme-ov-file)**: \"note that the **input image scale should be from 0-255**, which is different to the 0-1 scale in the previous resnet Torch models trained from scratch in fb.resnet.torch.\"\n",
    "* so, do not use any functions for preprocessing that scale to [0,1] without multiplying by 255 after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893065d2",
   "metadata": {},
   "source": [
    "Expand the collapsed columns to see the code used for model conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9637d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to convert the Keras model to ONNX format\n",
    "\n",
    "# import tf2onnx\n",
    "# import onnx\n",
    "# import tensorflow as tf \n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# import pickle\n",
    "\n",
    "# vgg16_path = Path(this_dir) / 'vgg16' \n",
    "# os.chdir(vgg16_path)\n",
    "# from vgg16_places_365 import VGG16_Places365\n",
    "\n",
    "# os.chdir(this_dir)\n",
    "\n",
    "# tf_model = VGG16_Places365() # 'places' weights are used by default \n",
    "\n",
    "# tf_model.build(input_shape=(None, 224, 224, 3)) # build the model with the input shape\n",
    "# out = tf_model.call(tf.random.normal(shape=(1, 224, 224, 3))) # call the model to initialize it\n",
    "\n",
    "# tf_out = out.numpy() # convert the output to a numpy array\n",
    "\n",
    "\n",
    "# # convert the Keras model to ONNX format\n",
    "# tf2onnx.convert.from_keras(tf_model, output_path=Path('vgg16', 'vgg16_places_365.onnx'), opset=13)\n",
    "\n",
    "# save the output to a file for later comparison\n",
    "# with open(vgg16_path / 'tf_out.pkl', 'wb') as f:\n",
    "#     pickle.dump(tf_out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "model = onnx.load(\"vgg16/vgg16_places_365.onnx\")\n",
    "onnx.checker.check_model(model)\n",
    "print(\"ONNX model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a78080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx2pytorch import ConvertModel\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "torch_model = ConvertModel(onnx.load(Path('vgg16', 'vgg16_places_365.onnx')))\n",
    "assert isinstance(torch_model, torch.nn.Module), \"Converted model is not a PyTorch module\"\n",
    "# save entire model \n",
    "torch.save(torch_model, Path('vgg16', 'pytorch' 'vgg16_places_365.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b4e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_in = torch.randn(1, 224, 224, 3) # a random input tensor\n",
    "with torch.no_grad():\n",
    "    pt_out = torch_model(test_in)\n",
    "\n",
    "pt_out = pt_out.cpu().numpy()  # convert the output to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c1e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity between the outputs of the TensorFlow and PyTorch models\n",
    "import pickle\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "tf_out = pickle.load(open(Path('vgg16') / 'tf_out.pkl', 'rb'))  # load the TensorFlow output\n",
    "cosine_similarity = 1 - cosine(tf_out.flatten(), pt_out.flatten())\n",
    "print(f\"Cosine similarity between TensorFlow and PyTorch outputs: {cosine_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c5cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained vgg16\n",
    "deep_model = torch.load('vgg16/pytorchvgg16_places_365.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a46db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name argument in get_features() specifies the dictionary key under which we will store our intermediate activations\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show features so we know what to register the hook on\n",
    "i=0\n",
    "pool_layers_names = {}\n",
    "for name, layer in deep_model.named_modules():\n",
    "    if isinstance(layer, torch.nn.modules.pooling.MaxPool2d):\n",
    "        # print(f\"Layer: {name}, Type: {type(layer)}\")\n",
    "        pool_layers_names[f'pool{i+1}'] = (name, layer)\n",
    "        i += 1\n",
    "\n",
    "pool_layers_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb985dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change pooling layer names to use Python ref names\n",
    "    # e.g. block1_pool is the new name/reference for the first pooling layer\n",
    "deep_model.block1_pool = deep_model._modules[pool_layers_names['pool1'][0]]\n",
    "deep_model.block2_pool = deep_model._modules[pool_layers_names['pool2'][0]]\n",
    "deep_model.block3_pool = deep_model._modules[pool_layers_names['pool3'][0]]\n",
    "deep_model.block4_pool = deep_model._modules[pool_layers_names['pool4'][0]]\n",
    "deep_model.block5_pool = deep_model._modules[pool_layers_names['pool5'][0]]\n",
    "\n",
    "# Note: the only immutable types in Python are: int, float, complex, bool, str, tuple, frozenset, bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ba4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register hooks\n",
    "layer_refs = [deep_model.block1_pool, deep_model.block2_pool,\n",
    " deep_model.block3_pool, deep_model.block4_pool, deep_model.block5_pool]\n",
    "\n",
    "for i, ref in enumerate(layer_refs):\n",
    "    ref.register_forward_hook(get_features(f'block{i+1}_pool'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9058ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from early, mid, and late layers\n",
    "def extract_deep_features(model, images, extraction_layer_name, device):\n",
    "    '''\n",
    "    Extracts features from a deep learning model for a batch of images.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: a pre-trained deep learning model\n",
    "    - images: a tensor of shape (N, C, H, W) where N is the number of images, C is the number of channels, H is the height, and W is the width\n",
    "    - extraction_layer_name: a string representing the name of the layer from which to extract features\n",
    "    Returns:\n",
    "    \n",
    "    '''\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    images = images.to(device)\n",
    "\n",
    "    idx = model.layer_names.index(extraction_layer_name)\n",
    "\n",
    "    # TODO: double check this works as expected\n",
    "    shortened_model = torch.nn.Sequential(*list(model.children())[:idx+1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = shortened_model(images)\n",
    "    \n",
    "    # important because the features might be on the GPU\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_layers_names = [..., ..., ...]\n",
    "deep_feats_dict = {}\n",
    "\n",
    "for layer in extraction_layers_names:\n",
    "    deep_feats_dict[layer] = extract_deep_features(torch_model, \n",
    "                                               images=..., \n",
    "                                               extraction_layer_name=..., \n",
    "                                               device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf013ee",
   "metadata": {},
   "source": [
    "## Feature Fusion and Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95447ba3",
   "metadata": {},
   "source": [
    "In the feature-fusion step, we concatenate together the traditional and deep features--one feature vector for each image. Concatenation was shown by this paper [insert link] to be the best operation to use for feature fusion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46978f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_features(trad_feats, deep_feats):\n",
    "    '''\n",
    "    Parameters: \n",
    "    - trad_feats: a tensor of shape (N, F) where N is the num of images and F is the feature dimension\n",
    "    - deep_feats: a tensor of shape (N, F) where N is the num of images and F is the feature dimension\n",
    "    Returns:\n",
    "        a tensor of shape (N, F) where N is the number of images and F is the total number of features (traditional + deep)\n",
    "    '''\n",
    "    # concat along axis 1 because axis 0 is the batch size and we want to concatenate features for each image\n",
    "    # dim=1 is the feature dimension\n",
    "    fusion = torch.cat((trad_feats, deep_feats), dim=1)\n",
    "\n",
    "    return fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_fused = fuse_features(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb2379",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4c523",
   "metadata": {},
   "source": [
    "PCA can be used as a feature reduction technique when you want to choose the features that result in **maximum variance**. It aims to reduce the feature space as much as possible while maintaining the most meaningful information. \n",
    "\n",
    "[IBM Tutorial](https://developer.ibm.com/tutorials/awb-reducing-dimensionality-with-principal-component-analysis/?sf185562769=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becba9a9",
   "metadata": {},
   "source": [
    "First, we should visualize the data in order to spot patterns, and this will also help decide the best value for PCA's n_components parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot\n",
    "# visualize based on class\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# first, create a DataFrame with the features and labels\n",
    "# TODO: this might not work with the many features after fusion\n",
    "feature_df = pd.DataFrame(feats_fused.numpy(), columns=[f'feat_{i}' for i in range(feats_fused.shape[1])])\n",
    "feature_df['label'] = ....numpy() # insert image train labels here\n",
    "\n",
    "# then visualize based on class and feature\n",
    "sns.scatterplot(data=feature_df, ..., hue='label' palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17aeb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# principal component index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c88009",
   "metadata": {},
   "source": [
    "[Tutorial for PCA and choosing n_components parameter](https://www.datasklr.com/principal-component-analysis-and-factor-analysis/principal-component-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec5e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scree plot / elbow plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91104520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA to scaled features \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def apply_PCA(features, n_components=...):\n",
    "    normalized_feats = cv2.normalize(features, None, 0, 1, cv2.NORM_MINMAX)\n",
    "    pca = PCA()\n",
    "    X_out = pca.fit_transform(normalized_feats)\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize output via scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9993049",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d68ec2",
   "metadata": {},
   "source": [
    "LDA is similar to PCA but is specifically used to find the features that promote **class separability**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195dfe52",
   "metadata": {},
   "source": [
    "[IBM LDA tutorial](https://developer.ibm.com/tutorials/awb-implementing-linear-discriminant-analysis-python/)\n",
    "\n",
    "[Other LDA tutorial](https://machinelearningmastery.com/linear-discriminant-analysis-for-dimensionality-reduction-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore data/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "def apply_LDA(features, n_components=...):\n",
    "    normalized_feats = cv2.normalize(features, None, 0, 1, cv2.NORM_MINMAX)\n",
    "    lda = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "    # TODO: make sure this is correct for dim reduction\n",
    "    X_out = lda.fit_transform(normalized_feats)\n",
    "\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ed3ed",
   "metadata": {},
   "source": [
    "## Classification - Random Forest Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_sci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
