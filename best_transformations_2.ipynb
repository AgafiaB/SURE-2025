{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4fdff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "import cv2 \n",
    "import skimage\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebb402f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset_without_emptyset(items):\n",
    "    '''\n",
    "    Returns the powerset of a list of items as a list of tuples, excluding the empty set\n",
    "    '''\n",
    "    combos = []\n",
    "    for i in range(len(items)):\n",
    "        combos.extend(list(combinations(items, len(items) - i)))\n",
    "    return combos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dce39f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.19269049e-03,  2.16750134e-07,  8.04738593e-13,  1.85217931e-13,\n",
       "        2.43328968e-26,  5.48469192e-17, -6.72401755e-26])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = datasets.OxfordIIITPet('/data', download=True, transform=None) # a dataset of PIL images \n",
    "\n",
    "# getting a pil image\n",
    "img, label = dataset.__getitem__(21)\n",
    "# Convert the PIL image to a NumPy array and then to grayscale for use with OpenCV functions\n",
    "im = np.array(img)\n",
    "\n",
    "if im.ndim == 3:\n",
    "    im_grey = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
    "else:\n",
    "    im_grey = imtransforms_to_use = [cv2.HuMoments, skimage.feature.graycomatrix, \n",
    "                     cv2.calcHist, skimage.feature.local_binary_pattern]\n",
    "    \n",
    "# Compute Hu Moments\n",
    "hu_moments = cv2.HuMoments(cv2.moments(im_grey)).flatten()\n",
    "hu_moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37b7cc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:15: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:12: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "<>:15: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "C:\\Users\\agafi\\AppData\\Local\\Temp\\ipykernel_22576\\3211328854.py:12: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(img.expand_dims(axis=0).shape)==3, f\"wrong shaped image: {img.shape}\")\n",
      "C:\\Users\\agafi\\AppData\\Local\\Temp\\ipykernel_22576\\3211328854.py:15: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(len(img.shape)==3, f\"wrong shaped image: {img.shape}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def ensure_rgb(img):\n",
    "    \"\"\"\n",
    "    Ensures the input image is a 3-channel RGB numpy array.\n",
    "    If the image is grayscale (2D), it is converted to RGB.\n",
    "    If the image is already RGB, it is returned unchanged.\n",
    "    \"\"\"\n",
    "    if isinstance(img, np.ndarray):\n",
    "        if img.ndim == 2:  # Grayscale\n",
    "            assert(len(img.expand_dims(axis=0).shape)==3, f\"wrong shaped image: {img.shape}\")\n",
    "            return cv2.cvtColor(img, cv2.COLOR_GRAY2RGB).expand_dims(axis=0)\n",
    "        elif img.ndim == 3 and img.shape[2] == 3:\n",
    "            assert(len(img.shape)==3, f\"wrong shaped image: {img.shape}\")\n",
    "            return img  # Already RGB\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported image shape for ensure_rgb: {}\".format(img.shape))\n",
    "    else:\n",
    "        raise TypeError(\"Input must be a numpy ndarray.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef6c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from sklearn.metrics.pairwise import nan_euclidean_distances\n",
    "import numpy as np\n",
    "import cv2\n",
    "import skimage\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO: check to ensure that the returned 2d data is correct for what you want \n",
    "\n",
    "def apply_transformations(images, combo):\n",
    "    features = []\n",
    "    for t in combo:\n",
    "        if t is cv2.HuMoments:\n",
    "            imgs_grey = [cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) if img.ndim == 3 else img for img in images]\n",
    "            feats = [cv2.normalize(cv2.HuMoments(cv2.moments(im)).flatten(), None, 0, 255, cv2.NORM_MINMAX) for im in imgs_grey]\n",
    "        elif t is skimage.feature.graycomatrix:\n",
    "            imgs_grey = [cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) if img.ndim == 3 else img for img in images]\n",
    "            dists, angles = [1], [0]\n",
    "            feats = []\n",
    "            for im in imgs_grey:\n",
    "                glcm = skimage.feature.graycomatrix(im, distances=dists, angles=angles, symmetric=True, normed=True)\n",
    "                diss = graycoprops(glcm, 'dissimilarity')\n",
    "                contrast = graycoprops(glcm, 'contrast')\n",
    "                cat = np.concatenate([diss, contrast], axis=1)\n",
    "                norm = cv2.normalize(cat, None, 0, 255, cv2.NORM_MINMAX).flatten()\n",
    "                feats.append(norm)\n",
    "        elif t is cv2.calcHist:\n",
    "            feats = [cv2.normalize(\n",
    "                        cv2.calcHist([img], [0,1,2], None, [8,8,8], [0,256]*3).flatten(),\n",
    "                        None, 0, 255, cv2.NORM_MINMAX).flatten() for img in images]\n",
    "        elif t is skimage.feature.local_binary_pattern:\n",
    "            imgs_grey = [cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) if img.ndim == 3 else img for img in images]\n",
    "            feats = []\n",
    "            for im in imgs_grey:\n",
    "                lbp = skimage.feature.local_binary_pattern(im, P=8, R=1)\n",
    "                hist, _ = np.histogram(lbp.ravel(), bins=10, range=(0, 10))\n",
    "                feats.append(hist.flatten())\n",
    "        else:\n",
    "            print(t)\n",
    "            raise ValueError(f'Unsupported transformation: {t}')\n",
    "        \n",
    "        features.append(np.stack(feats))\n",
    "\n",
    "        for i, feature in enumerate(features): \n",
    "            if len(feature.shape) > 1:\n",
    "                features[i] = feature.flatten()\n",
    "                \n",
    "    return np.concatenate(features, axis=0)\n",
    "\n",
    "def best_transformation(transformations, class1_imgs, class2_imgs):\n",
    "    '''\n",
    "    Parameters: \n",
    "        transformations - a list of transformation functions\n",
    "        class1_imgs, class2_imgs - lists of ndarray images\n",
    "\n",
    "    Returns:\n",
    "        The transformation or combination of transformations (as a tuple) that \n",
    "        produces the most class separability\n",
    "    '''\n",
    "    class1_imgs = [ensure_rgb(img) for img in class1_imgs]\n",
    "    class2_imgs = [ensure_rgb(img) for img in class2_imgs]\n",
    "    class1_imgs = [cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX) for img in class1_imgs]\n",
    "    class2_imgs = [cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX) for img in class2_imgs]\n",
    "\n",
    "    combos = powerset_without_emptyset(transformations)\n",
    "    best_combo = None\n",
    "    \n",
    "    score_dict = {}\n",
    "    for combo in tqdm(combos, total=len(combos)):\n",
    "        \n",
    "        transformed1 = np.expand_dims(apply_transformations(class1_imgs, combo), axis=1)\n",
    "        transformed2 = np.expand_dims(apply_transformations(class2_imgs, combo), axis=1)\n",
    "        \n",
    "        inter_score = nan_euclidean_distances(transformed1, transformed2).mean()\n",
    "        intra_score1 = nan_euclidean_distances(transformed1, np.flip(transformed1, axis=0))\n",
    "        intra_score2 = nan_euclidean_distances(transformed2, np.flip(transformed2, axis=0))\n",
    "        score_dict[combo] = {\"inter-score\": inter_score, \"intra-score class 1\": intra_score1, \"intra-score class 2\": intra_score2}\n",
    "\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4304246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels for all images\n",
    "labels = [dataset._labels[i] for i in range(len(dataset))]\n",
    "\n",
    "# Choose two class indices (e.g., 0 and 1)\n",
    "class1_idx = 0\n",
    "class2_idx = 1\n",
    "\n",
    "# Get indices for each class\n",
    "class1_indices = [i for i, l in enumerate(labels) if l == class1_idx]\n",
    "class2_indices = [i for i, l in enumerate(labels) if l == class2_idx]\n",
    "\n",
    "# Get images for each class (as PIL Images)\n",
    "class1_imgs = [dataset[i][0] for i in class1_indices]\n",
    "class2_imgs = [dataset[i][0] for i in class2_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5b441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# convert pil images to np.array\n",
    "# TODO: change back to 244,244\n",
    "class1_imgs_arr = [np.array(img.resize((24,24))) for img in class1_imgs]\n",
    "class2_imgs_arr = [np.array(img.resize((24,24))) for img in class2_imgs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07dfef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 24, 3)\n"
     ]
    }
   ],
   "source": [
    "print(class1_imgs_arr[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "090e0cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [cv2.HuMoments, skimage.feature.graycomatrix, cv2.calcHist, skimage.feature.local_binary_pattern]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfed8472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "best_combo = best_transformation(transforms, class1_imgs_arr, class2_imgs_arr)\n",
    "best_combo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b872bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize both images to (24, 24, 3) using cv2.resize, then flatten and reshape for distance calculation\n",
    "scores = []\n",
    "for img1, img2 in zip(class1_imgs_arr, class2_imgs_arr):\n",
    "    img1_resized = cv2.resize(img1, (24, 24)).reshape(-1, 1)\n",
    "    img2_resized = cv2.resize(img2, (24, 24)).reshape(-1, 1)\n",
    "    scores.append(nan_euclidean_distances(img1_resized, img2_resized ).mean())\n",
    "\n",
    "score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef42303",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in range(len(class1_imgs_arr) - 1):\n",
    "    img1_resized = cv2.resize(class1_imgs_arr[i], (24, 24)).reshape(-1, 1)\n",
    "    img2_resized = cv2.resize(class1_imgs_arr[i+1], (24, 24)).reshape(-1, 1)\n",
    "\n",
    "    scores.append(nan_euclidean_distances(img1_resized, img2_resized ).mean())\n",
    "\n",
    "score = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049dc3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2c406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fe4e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_sci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
